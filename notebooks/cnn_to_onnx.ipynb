{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f270b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "ONNX version: 1.19.1\n",
      "ONNX Runtime version: 1.22.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"../app/models\", exist_ok=True)\n",
    "\n",
    "PATH_TO_WEIGHTS = \"../outputs/cnn/fold_4_best_model.pth\"\n",
    "ONNX_PATH = \"../app/models/cnn_eye_classifier.onnx\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46bfcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(128 * (IMG_SIZE // 8) * (IMG_SIZE // 8), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767cabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: ../outputs/cnn/fold_4_best_model.pth\n",
      "Weights loaded successfully!\n",
      "Total parameters: 1,142,082\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = CNN().to(device)\n",
    "\n",
    "print(f\"Loading weights from: {PATH_TO_WEIGHTS}\")\n",
    "\n",
    "if not os.path.exists(PATH_TO_WEIGHTS):\n",
    "    raise FileNotFoundError(f\"Model file not found: {PATH_TO_WEIGHTS}\")\n",
    "\n",
    "try:\n",
    "    state_dict = torch.load(PATH_TO_WEIGHTS, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(\"Weights loaded successfully!\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4465c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to ONNX...\n",
      "ONNX model exported: ../app/models/cnn_eye_classifier.onnx\n",
      "File size: 4.36 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Exporting to ONNX...\")\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 64, 64, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        ONNX_PATH,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "file_size = os.path.getsize(ONNX_PATH) / (1024 * 1024)\n",
    "print(f\"ONNX model exported: {ONNX_PATH}\")\n",
    "print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd3824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying ONNX model...\n",
      "ONNX model valid\n",
      "\n",
      "Model Info:\n",
      "IR Version: 7\n",
      "Producer: pytorch\n",
      "Opset Version: 13\n",
      "\n",
      "Input:\n",
      "Name: input\n",
      "Shape: ['dynamic', 1, 64, 64]\n",
      "\n",
      "Output:\n",
      "Name: output\n",
      "Shape: ['dynamic', 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying ONNX model...\")\n",
    "\n",
    "try:\n",
    "    onnx_model = onnx.load(ONNX_PATH)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model valid\")\n",
    "    \n",
    "    print(f\"\\nModel Info:\")\n",
    "    print(f\"IR Version: {onnx_model.ir_version}\")\n",
    "    print(f\"Producer: {onnx_model.producer_name}\")\n",
    "    print(f\"Opset Version: {onnx_model.opset_import[0].version}\")\n",
    "    \n",
    "    print(f\"\\nInput:\")\n",
    "    for input_tensor in onnx_model.graph.input:\n",
    "        print(f\"Name: {input_tensor.name}\")\n",
    "        shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic' for dim in input_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"Shape: {shape}\")\n",
    "    \n",
    "    print(f\"\\nOutput:\")\n",
    "    for output_tensor in onnx_model.graph.output:\n",
    "        print(f\"Name: {output_tensor.name}\")\n",
    "        shape = [dim.dim_value if dim.dim_value > 0 else 'dynamic' for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"Shape: {shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ONNX model failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff51667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ONNX inference...\n",
      "\n",
      "Max difference between PyTorch and ONNX: 0.000000\n",
      "ONNX model matches PyTorch model\n",
      "\n",
      "PyTorch output shape: (1, 2)\n",
      "ONNX output shape: (1, 2)\n",
      "PyTorch prediction: 0\n",
      "ONNX prediction: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing ONNX inference...\")\n",
    "\n",
    "ort_session = ort.InferenceSession(ONNX_PATH)\n",
    "\n",
    "test_input = torch.randn(1, 1, 64, 64, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pytorch_output = model(test_input).cpu().numpy()\n",
    "\n",
    "onnx_output = ort_session.run(\n",
    "    None,\n",
    "    {\"input\": test_input.cpu().numpy()}\n",
    ")[0]\n",
    "\n",
    "diff = np.abs(pytorch_output - onnx_output).max()\n",
    "print(f\"\\nMax difference between PyTorch and ONNX: {diff:.6f}\")\n",
    "\n",
    "if diff < 1e-4:\n",
    "    print(\"ONNX model matches PyTorch model\")\n",
    "else:\n",
    "    print(\"Outputs differ significantly\")\n",
    "\n",
    "print(f\"\\nPyTorch output shape: {pytorch_output.shape}\")\n",
    "print(f\"ONNX output shape: {onnx_output.shape}\")\n",
    "print(f\"PyTorch prediction: {pytorch_output.argmax()}\")\n",
    "print(f\"ONNX prediction: {onnx_output.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e92fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running performance benchmark...\n",
      "\n",
      "==================================================\n",
      "INFERENCE PERFORMANCE (Average over 100 runs)\n",
      "==================================================\n",
      "PyTorch Model:       2.15 ms\n",
      "ONNX Standard:       0.23 ms (9.35x)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Running performance benchmark...\")\n",
    "\n",
    "num_iterations = 100\n",
    "test_input_np = test_input.cpu().numpy()\n",
    "\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iterations):\n",
    "        _ = model(test_input)\n",
    "pytorch_time = (time.time() - start_time) / num_iterations * 1000\n",
    "\n",
    "ort_session_std = ort.InferenceSession(ONNX_PATH)\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    _ = ort_session_std.run(None, {\"input\": test_input_np})\n",
    "onnx_time = (time.time() - start_time) / num_iterations * 1000\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"INFERENCE PERFORMANCE (Average over {num_iterations} runs)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"PyTorch Model:       {pytorch_time:.2f} ms\")\n",
    "print(f\"ONNX Standard:       {onnx_time:.2f} ms ({pytorch_time/onnx_time:.2f}x)\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
